---
title: 'Final-podcast-sound-on'
date: 2021-04-20
permalink: /posts/2021/20/final-podcast-sound-on/
tags:
  - 
---


<figure>
    <figcaption>Listen to "Sound On":</figcaption>
    <audio
        controls
        src="/files/alljoined.mp3">
            Your browser does not support the
            <code>audio</code> element.
    </audio>
</figure>

https://magenta.tensorflow.org/assets/piano_transformer/clair_de_lune_continuation.mp3

# Sound On
----------------------------------------------------------------------------------------------------------------------------
<figure>
    <figcaption>Listen to https://magenta.tensorflow.org/piano-transformer</figcaption>
    <audio
        controls
        src="https://magenta.tensorflow.org/assets/piano_transformer/clair_de_lune_continuation.mp3">
            Your browser does not support the
            <code>audio</code> element.
    </audio>
</figure>
What you just heard sounded like music, it had rhythm and tonal structures which I for one can identify as music. What isn’t clear is whether I can identify with it. The reason I claim that is because the music just played was made by a Machine Learning Algorithm Developed by Google’s Magenta project. The project has actually made a range of Algorithms which do different things for music. There are however other projects outside of Google, such as DeepBach, Muse Gan, and another called Aiva have also made machines make some musical sounding sounds, although some are not as successfully indiscernible from human made music as others. In this recording I will review some of the shared Machine Learning algorithms used by the different projects and what they might mean for the future of Music and Machine learning.

https://www.youtube.com/watch?v=QiBM7-5hA6o
<iframe width="864" height="486" src="https://www.youtube.com/embed/QiBM7-5hA6o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
The first algorithm I’d like to focus on is called DeepBach. As the name suggests, it’s music is related to Bach’s music. The Music is related in the way that any sound the creator’s model learned to make, was learned by processing Bach’s music. In terms of the algorithms it uses to learn, DeepBach’s creators avoided using a single Recurrent Neural Networks because they claimed that although such a technique might work on piano roll representations of music, they “are too general to capture the specificity of Bach chorales.” (DeepBach 2).  Interestingly they did not abandon Recurrent Neural Networks completely because they used it to tackle the problem of uni-directionality by doubling up with two deep Recurrent Neural Networks, one which is trained on previous information, one trained on future information. Connecting the past and future signals is a single layer perceptron that is involved with the present. The sound from DeepBach --in my opinion-- is quite similar to Bach but listeners of DeepBach may feel that they miss out on a sense of novelty from true Bach music. The model may be perceiving Bach’s innovativeness as a noise which can be smoothed out from the overall trajectory of the voices. 

<figure>
    <figcaption>Listen to https://salu133445.github.io/musegan/</figcaption>
    <audio
        controls
        src="https://salu133445.github.io/musegan/audio/best_samples.mp3">
            Your browser does not support the
            <code>audio</code> element.
    </audio>
</figure>

Another model worth mentioning is called MuseGan. I think it’s worth mentioning because the makers of MuseGan approached the task of measuring whether what they made is music by using objectively measurable characteristics. They also used people to listen and rate the sounds just as DeepBach had done, but they also tried to lay the groundwork to understanding whether music can objectively be categorized with values like TD (tonal distance) to verify that their model was indeed capturing inter-track harmonic relations(MUSEGAN 5). Their other metrics such as QN (qualified notes) also suggest a sort of objectivity to what a person would have arranged in terms of the length which a note would be played(MUSEGAN 5). MuseGan uses a model called a Generative Adversarial Network (GAN) which is trained to generate some true selection of states by also training a neural network to discern what is the “true” signal and what is the generated signal.

The models described thus far are interesting because they differ and produce different types of sound. On the one hand, Bach’s use of Recurrent Neural Network’s display a temporal coherence which is expected of those types of neural networks. On the other hand MuseGan’s GAN offers a less interesting sound but the objective approach from it’s creators helps to pave a way for a new way of thinking about music and art through machines. Both of those types of models have been adopted by Magenta. Magenta from Google is as they put it, a “research project exploring the role of machine learning as a tool in the creative process.” (MAGENTA https://magenta.tensorflow.org/ ). The description helps to explain the broad range of research happening within Magenta; from generating drum beats based on the RNN model which humans can tune the output of with sliders like “tempo”, “swing”, and “temperature” (MAGENTA NEURAL DRUM MACHINE https://codepen.io/teropa/full/JLjXGK) to generating novel timbres using GANs (MAGENTA https://ganharp.ctpt.co/). Magenta seems to be exploring many avenues for the creative process with respect to music. As an interesting note, Magenta also decided to use another type of architecture called a Transformer which actually improved their musical coherence over time when compared to RNN based samples which I have seen; it is what you heard in the opening of this podcast (MAGENTA https://magenta.tensorflow.org/piano-transformer).

I find Google’s breadth of models and successes to be interesting because they also publish all their models and research online for everyone to see (MAGENTA https://magenta.github.io/magenta-js/music/). That is perhaps because they wish for the technology to improve, but it may also be because some of the best sounding samples from some of the models get seeded with a short intro from something a human made and so they hope to avoid legal disputes related to any music they generate. 



https://www.youtube.com/watch?v=Emidxpkyk6o
<iframe width="864" height="486" src="https://www.youtube.com/embed/Emidxpkyk6o" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
In this way I find a lot of controversy with the last music generation example I found to be worth mentioning, AIVA. Although the music I hear from AIVA’s compositions is very much like listening to a performance from humans, I am unable to find how it is able to generate hours of music, in different genres, and of such high musical quality. Unlike the previous examples given, who present the methods through which they generate their music, AIVA is quite secretive in its methods. I must admit that hearing it’s quality is on par with Magenta’s Transformer which performs best when seeded a sample of music. So, I only hope that should AIVA’s music be sold to game developers as they had intended, that the music can in some way be verified to be original or non-original so that royalties can be paid.


As we near the end of this recording I want to remind the listener that in order to understand the future of music and Machine Learning we should consider that: Machine Learning is capable of helping make time based information. Because transformers have shown great success in Natural language processing, I was not surprised to see their efficacy in generating sounds with high musical quality. Recurrent Neural Networks are in some ways related to Transformers in that they too form a strong basis for time-based information, but they have fallen to the might of Transformers in other research areas, so I am again not entirely surprised to see Transformers behaving best with musical research. The generative aspects of sound which are perceived in a much shorter period of time however seemed to show great success for GANs and RNNs. I am not entirely sure where music will go after machine learning algorithms perfect the rhythms of the past, but I do know that people will have to innovate on objective and subjective spheres in order to compete with the novel sounds machine learning algorithms can produce and repeat.







```

Bibliography

“AIVA: The Artificial Intelligence Composing Classical Music.” Silicon Luxembourg, 8 Apr. 2019, www.siliconluxembourg.lu/aiva-the-artificial-intelligence-composing-classical-music/.

“Demos.” Magenta, magenta.tensorflow.org/demos.

Dong, Hao-Wen, et al. “MuseGAN: Multi-Track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment.” ArXiv.org, 24 Nov. 2017, arxiv.org/abs/1709.06298.

“Generating Piano Music with Transformer.” Magenta, 16 Sept. 2019, magenta.tensorflow.org/piano-transformer.

Hadjeres, Gaëtan, et al. “DeepBach: a Steerable Model for Bach Chorales Generation.” ArXiv.org, 17 June 2017, arxiv.org/abs/1612.01010.

Kalahiki, Chris. “Beethoven, Picasso, and Artificial Intelligence.” Medium, Towards Data Science, 9 June 2018, towardsdatascience.com/beethoven-picasso-and-artificial-intelligence-caf644fc72f9.

“Pierre Barreau - AI Created Hans Zimmer Compositions, Deep Fakes the Future of Art.” YouTube, YouTube, 19 July 2019, www.youtube.com/watch?v=I6aGgVCLYEE.

salu133445. MuseGAN, salu133445.github.io/musegan/.

SonyCSLParis. “DeepBach: Harmonization in the Style of Bach Generated Using Deep Learning.” YouTube, YouTube, 13 Dec. 2016, www.youtube.com/watch?v=QiBM7-5hA6o.

```

